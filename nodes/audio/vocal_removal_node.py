import os
# NumPy 2.x compatibility fix
import numpy as np
if not hasattr(np, 'float'):
    np.float = float
    np.int = int
    np.complex = complex
    np.bool = bool
    
import audio_separator.separator as uvr

# Add engine path for imports
import sys
current_dir = os.path.dirname(__file__)
engines_dir = os.path.dirname(os.path.dirname(current_dir))
rvc_impl_path = os.path.join(engines_dir, "engines", "rvc", "impl")
if rvc_impl_path not in sys.path:
    sys.path.insert(0, rvc_impl_path)

# AnyType for flexible input types
class AnyType(str):
    def __ne__(self, __value: object) -> bool:
        return False

from rvc_audio import audio_to_bytes, save_input_audio, load_input_audio, get_audio
import folder_paths
from rvc_utils import get_filenames, get_hash, get_optimal_torch_device
from lib import karafan
from rvc_downloader import KARAFAN_MODELS, MDX_MODELS, RVC_DOWNLOAD_LINK, VR_MODELS, ZFTURBO_MODELS, ZFTURBO_DOWNLOAD_LINK, download_file

# Define paths
BASE_CACHE_DIR = folder_paths.get_temp_directory()
BASE_MODELS_DIR = folder_paths.models_dir

temp_path = folder_paths.get_temp_directory()
cache_dir = os.path.join(BASE_CACHE_DIR,"uvr")
device = get_optimal_torch_device()
is_half = True

class VocalRemovalNode:
    
    @classmethod
    def NAME(cls):
        return "ü§ê Noise or Vocal Removal"
 
    def __init__(self):
        pass
    
    @classmethod
    def INPUT_TYPES(cls):

        # Add ZFTurbo SOTA models to the list
        zfturbo_model_names = [model_path for _, model_path in ZFTURBO_MODELS]
        
        # Search both TTS and legacy paths for models
        tts_models = get_filenames(root=os.path.join(BASE_MODELS_DIR, "TTS"),format_func=lambda x: f"{os.path.basename(os.path.dirname(x))}/{os.path.basename(x)}",name_filters=["UVR","MDX","karafan","SCNET","MDX23C","MELBAND"])
        legacy_models = get_filenames(root=BASE_MODELS_DIR,format_func=lambda x: f"{os.path.basename(os.path.dirname(x))}/{os.path.basename(x)}",name_filters=["UVR","MDX","karafan","SCNET","MDX23C","MELBAND"])
        
        model_list = (MDX_MODELS + VR_MODELS + KARAFAN_MODELS + zfturbo_model_names + tts_models + legacy_models)
        model_list = list(set(model_list)) # dedupe
        
        # Filter out non-model files (JSON configs, etc.)
        model_extensions = ['.pth', '.ckpt', '.onnx', '.pt', '.safetensors']
        model_list = [model for model in model_list if any(model.lower().endswith(ext) for ext in model_extensions)]

        return {
            "required": {
                "audio": ("AUDIO", {
                    "tooltip": "Input audio for vocal/instrumental separation. Standard ComfyUI AUDIO format."
                }),
                "model": (model_list,{
                    "default": "MDXNET/UVR-MDX-NET-vocal_FT.onnx",
                    "tooltip": """üéµ AI AUDIO SEPARATION & PROCESSING

üèÜ TOP MODELS (2024-2025):
‚Ä¢ UVR-MDX-NET-vocal_FT.onnx - ‚≠ê DEFAULT (Reliable, Fast)
‚Ä¢ model_bs_roformer_ep_317_sdr_12.9755.ckpt - üéµ ADVANCED (12.98 SDR)

‚ö†Ô∏è EXPERIMENTAL (NOT WORKING):
‚Ä¢ denoise_mel_band_roformer_sdr_27.99.ckpt - ü•á BEST DENOISING (27.99 SDR) - ‚ùå Architecture mismatch
‚Ä¢ model_vocals_mdx23c_sdr_10.17.ckpt - üé§ BEST VOCALS (10.17 SDR) - ‚ùå Tensor alignment errors
‚Ä¢ model_scnet_xl_ihf_sdr_10.08.ckpt - SCNet SOTA (10.08 SDR) - ‚ö†Ô∏è Audio buzzing artifacts

üéØ QUICK START:
‚Ä¢ üèÜ Best Overall: UVR-MDX-NET-vocal_FT.onnx (Proven reliability)
‚Ä¢ üéµ Vocal Extraction: UVR-MDX-NET-vocal_FT (reliable, fast)
‚Ä¢ üîß Light Denoising: UVR-DeNoise + gentle aggressiveness (5-8)
‚Ä¢ üè† Beginner: HP5-vocals+instrumentals + moderate aggressiveness (10)

‚ö†Ô∏è SPECIAL MODELS:
‚Ä¢ UVR-DeNoise - NOISE REMOVAL: "remaining" = clean audio ‚úÖ
‚Ä¢ UVR-DeEcho-DeReverb - ECHO REMOVAL: "remaining" = dry audio ‚úÖ

üìñ Complete guide with all models & workflows: docs/VOCAL_REMOVAL_GUIDE.md"""
                }),
            },
            "optional": {
                "use_cache": ("BOOLEAN",{
                    "default": True,
                    "tooltip": """üöÄ CACHING SYSTEM

Enables intelligent caching of separation results for faster processing:
‚Ä¢ ‚úÖ ON (Recommended): Saves results to disk, dramatically speeds up repeated processing of same audio/model combinations
‚Ä¢ ‚ùå OFF: Always processes from scratch, uses more time but ensures fresh results

üí° Cache includes model, aggressiveness, format, and audio content in hash
üîÑ Automatically invalidates when any parameter changes
üíæ Cached files stored in organized folder structure for easy management"""
                }),
                "aggressiveness":("INT",{
                    "default": 10, 
                    "min": 0, #Minimum value
                    "max": 20, #Maximum value
                    "step": 1, #Slider's step
                    "display": "slider",
                    "tooltip": """üéöÔ∏è SEPARATION AGGRESSIVENESS (0-20)

Controls separation strength for VR architecture models (HP5, DeNoise, DeEcho, etc.):

üìä RECOMMENDED VALUES:
‚Ä¢ 0-5: Gentle separation, preserves more original audio quality
‚Ä¢ 6-10: ‚≠ê BALANCED (Default: 10) - Good separation with minimal artifacts
‚Ä¢ 11-15: Aggressive separation, may introduce artifacts but better isolation
‚Ä¢ 16-20: Maximum aggression, highest separation but potential quality loss

üéØ USE CASES:
‚Ä¢ üé§ Karaoke Creation: 12-15 (more aggressive)
‚Ä¢ üéµ Vocal Extraction: 8-12 (balanced)
‚Ä¢ üéº Preserve Music Quality: 5-8 (gentle)
‚Ä¢ üîß Problem Audio: 15-20 (maximum effort)

‚ö†Ô∏è NOTE: Only affects VR Architecture models (HP5, DeNoise, DeEcho). Advanced models (UVR-MDX-NET, bs_roformer, MDX23C) ignore this setting.
üí° Higher values = stronger vocal/instrumental separation but may affect audio quality"""
                }),
                "format":(["wav", "flac", "mp3"],{
                    "default": "flac",
                    "tooltip": """üéµ OUTPUT AUDIO FORMAT

Selects the audio format for separated stems:

üèÜ QUALITY RANKING:
‚Ä¢ üìÄ FLAC: ‚≠ê BEST - Lossless compression, perfect quality, larger files
‚Ä¢ üéµ WAV: Uncompressed, perfect quality, largest files  
‚Ä¢ üéß MP3: Lossy compression, smaller files, slight quality loss

üíº PROFESSIONAL USE: FLAC (default)
üöÄ FAST WORKFLOW: MP3 (smaller files, faster I/O)
üéØ MAXIMUM QUALITY: WAV (no compression)

üìä FILE SIZE COMPARISON (typical 4-minute song):
‚Ä¢ WAV: ~40MB per stem
‚Ä¢ FLAC: ~20MB per stem  
‚Ä¢ MP3: ~4MB per stem

üí° All formats support the full separation quality - format only affects storage and compatibility"""
                }),
            }
        }

    RETURN_TYPES = ("AUDIO", "AUDIO")
    RETURN_NAMES = ("extracted voice/noise/echo", "remaining")

    FUNCTION = "split"

    CATEGORY = "üéµ TTS Audio Suite/Audio"

    def split(self, audio, model, use_cache=True, aggressiveness=10, format='flac'):
        filename = os.path.basename(model)
        subfolder = os.path.dirname(model)
        
        # Try TTS organization first, then legacy
        tts_model_path = os.path.join(BASE_MODELS_DIR, "TTS", subfolder, filename)
        legacy_model_path = os.path.join(BASE_MODELS_DIR, subfolder, filename)
        
        if os.path.isfile(tts_model_path):
            model_path = tts_model_path
        elif os.path.isfile(legacy_model_path):
            model_path = legacy_model_path
        else:
            # Model not found, will download to TTS path
            model_path = tts_model_path
        
        if not os.path.isfile(model_path):
            # Check if it's a ZFTurbo model
            zfturbo_model = next((download_path for download_path, model_path_check in ZFTURBO_MODELS if model_path_check == model), None)
            
            if zfturbo_model:
                download_link = f"{ZFTURBO_DOWNLOAD_LINK}{zfturbo_model}"
                print(f"üì• Downloading SOTA model from ZFTurbo repository: {filename}")
            else:
                download_link = f"{RVC_DOWNLOAD_LINK}{model}"
                print(f"üì• Downloading model from RVC Studio: {filename}")
            
            params = model_path, download_link
            if download_file(params): print(f"‚úÖ Successfully downloaded: {model_path}")
        
        input_audio = get_audio(audio)
        hash_name = get_hash(model, aggressiveness, format, audio_to_bytes(*input_audio))
        audio_path = os.path.join(temp_path,"uvr",f"{hash_name}.wav")
        primary_path = os.path.join(cache_dir,hash_name,f"primary.{format}")
        secondary_path = os.path.join(cache_dir,hash_name,f"secondary.{format}")
        primary=secondary=None

        if os.path.isfile(primary_path) and os.path.isfile(secondary_path) and use_cache:
            print(f"üöÄ Using cached separation results for faster processing")
            primary = load_input_audio(primary_path)
            secondary = load_input_audio(secondary_path)
        else:
            if not os.path.isfile(audio_path):
                os.makedirs(os.path.dirname(audio_path),exist_ok=True)
                print(save_input_audio(audio_path,input_audio))
            
            print(f"üéµ Starting vocal separation with {os.path.basename(model)}")
            try: 
                if "karafan" in model_path: # try karafan implementation
                    print(f"üîß Using Karafan separation engine")
                    primary, secondary, _ = karafan.inference.Process(audio_path,cache_dir=temp_path,format=format)
                else: # try python-audio-separator implementation
                    print(f"üîß Using Audio-Separator engine")
                    model_dir = os.path.dirname(model_path)
                    model_name = os.path.basename(model_path)
                    vr_params={"batch_size": 4, "window_size": 512, "aggression": aggressiveness, "enable_tta": False, "enable_post_process": False, "post_process_threshold": 0.2, "high_end_process": "mirroring"}
                    mdx_params={"hop_length": 1024, "segment_size": 256, "overlap": 0.25, "batch_size": 4}
                    model = uvr.Separator(model_file_dir=os.path.join(BASE_MODELS_DIR,model_dir),output_dir=temp_path,output_format=format,vr_params=vr_params,mdx_params=mdx_params)
                    model.load_model(model_name)
                    output_files = model.separate(audio_path)
                    primary = load_input_audio(os.path.join(temp_path,output_files[0]))
                    secondary = load_input_audio(os.path.join(temp_path,output_files[1]))
            except Exception as e: # try RVC implementation
                print(f"‚ö†Ô∏è Primary engine failed (model not in supported list), switching to RVC fallback engine...")
                print(f"üí° This is normal - downloading and using model with RVC implementation")
                
                from uvr5_cli import Separator
                model = Separator(
                    model_path=model_path,
                    device=device,
                    is_half="cuda" in str(device),
                    cache_dir=cache_dir,
                    agg=aggressiveness
                    )
                primary, secondary, _ = model.run_inference(audio_path,format=format)
                print(f"‚úÖ RVC fallback completed successfully!")
            finally:
                if primary is not None and secondary is not None and use_cache:
                    print(f"üíæ Caching results for faster future processing")
                    print(save_input_audio(primary_path,primary))
                    print(save_input_audio(secondary_path,secondary))

                if os.path.isfile(primary_path) and os.path.isfile(secondary_path) and use_cache:
                    primary = load_input_audio(primary_path)
                    secondary = load_input_audio(secondary_path)
        
        # Convert back to ComfyUI formats
        def to_audio_dict(audio_data, sample_rate):
            import torch
            if isinstance(audio_data, np.ndarray):
                if audio_data.ndim == 1:
                    waveform = torch.from_numpy(audio_data).float().unsqueeze(0).unsqueeze(0)  # [1, 1, samples]
                else:
                    waveform = torch.from_numpy(audio_data).float().unsqueeze(0)  # [1, channels, samples]
            else:
                waveform = torch.tensor(audio_data).float().unsqueeze(0).unsqueeze(0)
            
            return {
                "waveform": waveform,
                "sample_rate": sample_rate
            }
        
        # Some models return vocals/instrumentals in opposite order
        model_name = filename.lower()  # Use original filename, not the reassigned model object
        
        # Models that typically return inverted outputs 
        if ("roformer" in model_name or "bs_roformer" in model_name or 
            ("karaoke" in model_name and "hp" in model_name) or
            "deecho" in model_name or "dereverb" in model_name):
            # Swap outputs for these models
            print(f"üîÑ Model with inverted outputs detected - swapping (primary=instrumentals, secondary=vocals)")
            return (to_audio_dict(secondary[0], secondary[1]), to_audio_dict(primary[0], primary[1]))  # extracted=vocals, remaining=instrumentals
        else:
            # Standard order for most models
            return (to_audio_dict(primary[0], primary[1]), to_audio_dict(secondary[0], secondary[1]))  # extracted=vocals, remaining=instrumentals